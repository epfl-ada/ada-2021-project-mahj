{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a0032c",
   "metadata": {},
   "source": [
    "# Le plan: \n",
    "\n",
    "When one wants to get information \n",
    "## 1. Blabla intro du sujet TODO\n",
    "\n",
    "## 2. What is our goal?\n",
    "\n",
    "## 3. How do we achieve it (in short)?\n",
    "~Use Quotebank + Unsupervised learning\n",
    "\n",
    "## 4. Data story\n",
    "\n",
    "## 5. Conclusions: \n",
    "### 5.1 What did we find?\n",
    "### 5.2 What's next/would be interesting to do\n",
    "\n",
    "### somewhere:\n",
    "Indicate that we alternatively the terms speaker/citee/quotee, journal/media/newspaper, and group/consortium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9735eba",
   "metadata": {},
   "source": [
    "# 1. Intro du sujet: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699fef9",
   "metadata": {},
   "source": [
    "# 2. Our goal:\n",
    "We aim to show that we can recognize which media group a journal belongs to, using some automatic clustering algorithm using very little data. As a result, we will have shown a lack of diversity of these journals, thus showing that if you want to be well-informed, simply reading different newspapers is not enough. One needs to pay attention to more than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e80891",
   "metadata": {},
   "source": [
    "# 3. How we achieve that:\n",
    "\n",
    "\\[For a detailed description of the process, keep reading, or better, go see our [notebook](https://github.com/epfl-ada/ada-2021-project-mahj)\\].\n",
    "\n",
    "### 3.1: Overview of the process\n",
    "\n",
    "We need some *features* which allow us to distinguish between diferent media. To do so we use the [Quotebank](https://dlab.epfl.ch/people/west/pub/Vaucher-Spitz-Catasta-West_WSDM-21.pdf) database, which is a dataset of  speaker-attributed quotations that were extracted from diverse English news articles[^1].\n",
    "\n",
    "[^1]: https://zenodo.org/record/4277311\n",
    "\n",
    "We then augment this dataset using [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) to get the features we deemed necessary for our analysis.\n",
    "\n",
    "### 3.2: Explanation of some of our choices (**TODO: and limitations?**)\n",
    "We have chosen to only focus on one year (namely 2020), as we expect the topics of interest, as well as the prominent people to vary from one year to another.  \n",
    "We expect basically all the media to talk about the coronavirus, so we don't expect this subject to be a \"recognizing/dividing\" criterion. We will have to make sure it does not overshadow other criteria. \"blabla mitigate\"\n",
    "\n",
    "\n",
    "**TODO: commenter la taille  É N O R M E  du dataset qu'on a?**  \n",
    "**TODO: refine last paragraph**\n",
    "After getting all those features, we use [Natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing)-techniques (or NLP) to cluster media, and see what we find.\n",
    "**TODO: expliquer pk on ne prend qu'une année. E.g. pcque les news changent bcp entre les années, esp with covid / different presidents, ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9392c0b",
   "metadata": {},
   "source": [
    "# 4. But more precisely, what did you do?:\n",
    "**TODO: est-ce qu'on veut tenter du test-training? parce que jusqu'ici on ne fait que du training, et je me demande s'ils vont vouloir pénaliser ça.**  \n",
    "**TODO: ptetre train sur les $x$ premiers mois, puis test sur les $12-x$ derniers mois, comme ça on justifie l'utilité: en s'entraînant avec la data du début de l'année on a des résultats pour la fin de l'année.**. Pb avec ça: yearly events\n",
    "\n",
    "**TODO sur les graphes préciser que l'axe indiqué en premier correspond aux abscisses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763e549",
   "metadata": {},
   "source": [
    "### 4.1: data-extension\n",
    "Well for starters, the data we extract from the Quotebank-dataset look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d490d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici quelques extraits de la data de quotebank avec juste les attributs qu'on a gardés\n",
    "\n",
    "# Ou ptetre ?:\n",
    " # 1: original\n",
    " # 2: ce qu'on a gardé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f3da9",
   "metadata": {},
   "source": [
    "As you can see, the owners of the media are not indicated here, so we need to extend our dataset with these features.  \n",
    "If we can't automatically find a journal's parent group, we decide to throw it away.\n",
    "We also do so if the journals are lonely children. Indeed, we seek (**todo: above all/firstly**) to cluster them according to their parent groups, so isolated media are irrelevant.\n",
    "\n",
    "**TODO? \"see file Wikidata_scraping.ipynb\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02032bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of wikidata code for scraping?\n",
    "# Exemple de à quoi ressemble les infos obtenues / la nouvelle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5652123",
   "metadata": {},
   "source": [
    "### 4.2: Data processing[^1]?\n",
    "Since for each we have both quotes, and quotees (i.e. the people quoted), it seems natural to try and cluster them according to these 2 features. Let us then try 2 different methods:\n",
    " * First, cluster according to which speakers are cited in it.\n",
    " * Second, cluster according to the text content of the extracted quotes.\n",
    " \n",
    "**TODO: **\n",
    "**TODO: do we speak about the distance metrics?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5c6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1, do a small graph like:\n",
    "# journal i:\n",
    "  # speaker 1\n",
    "  # speaker 2\n",
    "  # ...\n",
    "  # speaker n\n",
    "\n",
    "# For 2, do sth like:\n",
    "# journal j:\n",
    "  # quote 1\n",
    "  # quote 2\n",
    "  # ...\n",
    "  # quote m\n",
    "\n",
    "# Maybe 1 on the left, and 1 on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e75c69",
   "metadata": {},
   "source": [
    "For both these features, we will first use the [Term Frequency-Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) method (TF-IDF for short) (**todo: better name?**) before proceeding to further analysis.  \n",
    "In a nutshell, this method allows one to measure the importance of a term[^1] to a document[^2] in a collection[^3] based on its number of apparitions.\n",
    "\n",
    "[^1]: Here a citee, or a word\n",
    "[^2]: Here a media\n",
    "[^3]: Here, all the media we have kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310d3b6",
   "metadata": {},
   "source": [
    "**TODO: maybe a bit long?**\n",
    "Basically this method gives to each word a weight:\n",
    " * *proportional* to its number of occurences in d (term frequency)\n",
    " * and *inversely proportional* to its presence in the corpus (inverse document frequency)[^1]\n",
    "\n",
    "The reasoning is that we use the **IDF** to **find** words **unique** to each document, or **remove recurrent** words, which we find in basically every document, and therefore don't bring any information (e.g. stopwords such as 'the' in a text).  \n",
    "We use the **TF** to find words that **appear often**, as they may be **characteristic** of a particular document. For example the word 'brain' may often appear in a neuroscience paper, while not so much in a geography one.\n",
    "\n",
    "[^1]: to quantify the 'presence' of a word in the corpus, we simply count the number of documents in which it occurs. We don't take its occurences per document into account here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ec21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrations: les formes des 2 matrices obtenues, peu importe les coefs\n",
    "\n",
    "# Though on peut mettre une petit flèche vers\n",
    "# la matrice journal-speaker[i, j] par ex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b8ea4",
   "metadata": {},
   "source": [
    "### 4.3: Data analysis (the heart of our problem)\n",
    "**TODO: see file \"Analysis.ipynb\" for more details**\n",
    "\n",
    "Now that we have processed our data, we need to visualize it.  \n",
    "The problem is that the data (our TF-IDF matrices) is inherently high-dimensional, so we as humans can't directly visualize it. That is why we transform our matrices using [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD).\n",
    "\n",
    "This transformation should allow us to extract \"[topics](https://en.wikipedia.org/wiki/Topic_model)\", which correspond to the (orthonormal) directions of highest variation and their importance. We can then view the differences between different media along one such topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1164d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some illustration of SVD, or maybe of PCA to get the idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75a4a8",
   "metadata": {},
   "source": [
    "#### 4.3.1: The speakers\n",
    "Let us apply this technique on the speakers matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e3bd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration des projections qu'on a au départ, avec la séparation géographique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63082a",
   "metadata": {},
   "source": [
    "Using the top speakers per axe, we identify the *topics* of the axes:  \n",
    "**TODO: we have min and max, but isn't the axis's name base on max ?**\n",
    "**e.g. first axis is \"speaker frequency\"\n",
    "\n",
    "| Axis | Max                 | Min                     |\n",
    "|------|---------------------|-------------------------|\n",
    "| 0    | Frequent speakers   | Non Frequent speakers   |\n",
    "| 1    | US speakers         | UK speakers             |\n",
    "| 2    | Australian speakers | US speakers             |\n",
    "| 3    | Canadian speakers   | US politician speakers  |\n",
    "| 4    | Basketball speakers | US politicians speakers |\n",
    "\n",
    "As you can see, the speakers are very different in newspapers from different countries which results in clustering mostly based on the country of origin of the newspapers.  \n",
    "It makes sense: UK-media will cite more UK-speakers, while USA-media will cite more USA-speakers. \n",
    "\n",
    "So our clustering works (for example it is easy to recognize journals from the media group *News Corp Australia*), but it is not enough yet. \n",
    "Indeed, even if one is aware they should get their information from different media, are they really that likely to get it from different countries' media? Of course not, and that might be especially true for American people (**TODO: is there some study that says american are self centered? or at least we can find some about the nb of languages they know.**). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc297e73",
   "metadata": {},
   "source": [
    "Then let us see if we can differentiate the media *inside one country!*. We will focus on the USA since it is the country with the most data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac28691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the list of plots \"speaker with only USA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b6715",
   "metadata": {},
   "source": [
    "Again we can try and extract the *topics* from the list of top speakers per axe:\n",
    "\n",
    "| Axis | Max                             | Min                                              |\n",
    "|------|---------------------------------|--------------------------------------------------|\n",
    "| 0    | Frequent speakers               | Non Frequent speakers                          |\n",
    "| 1    | Sport coaches of Los Angeles    | West coast personalities                       |\n",
    "| 2    | Sport coaches of San Francisco  | Sport coaches/DJ of Los Angeles                |\n",
    "\n",
    "We also see (in bright green) that journals from *Townsquare Media* explain most of the variance of several axes, namely the 3rd and the 4th.  \n",
    "Hence although we can visually recognize media from this group, it might be interesting to redo our analysis while removing media from this group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbdbbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les nouveaux plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efeca1",
   "metadata": {},
   "source": [
    "Clearly we can spot some clusters when projecting on the first few axes. The purple and yellow ones are especially well separated. \n",
    "**TODO: j'ai des pb pour run avec le .csv, donc je peux notamment pas voir à quel groupe ou quel topic ça correspond**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d27244",
   "metadata": {},
   "source": [
    "**TODO:** semi-conclusion on the efficiency of this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751981b0",
   "metadata": {},
   "source": [
    "#### 4.3.2:  The quotes\n",
    "\n",
    "Now let us apply this technique on the words' matrix (constructed with the words from the quotes).  \n",
    "For starters we will consider all the quotes, and not only the ones in US newspapers. We expect there is still some geographical distinction --due for example to cultural differences, or diverging points of interest (e.g. American football is popular in the USA, but not much in other English speaking countries)--  \n",
    "\n",
    "But we don't expect them to be as obvious as with the speakers' matrix, since we expect more recoupement between journals of different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a856996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration des projections qu'on obtient\n",
    "\n",
    "# ptetre un zoom sur le 0-4? ou un où on voit le mieux une séparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65716287",
   "metadata": {},
   "source": [
    "#### TODO: why is it called \"US health\" and not just \"health\" in the notebook \"Analysis\"?\n",
    "\n",
    "Again, using the top words per axe we can identify the topics corresponding to the axes:\n",
    "\n",
    "| Axis | Max            | Min                   |\n",
    "|------|----------------|-----------------------|\n",
    "| 0    | Frequent words | Non frequent words    |\n",
    "| 1    | Sports[^1]     | US health             |\n",
    "| 2    | UK **TODO: verify**             | Not clear. Some terms related to music though|\n",
    "| 3    | Music          | Sports                |\n",
    "| 4    | Canada         | US politics **TODO: verify**           |\n",
    "\n",
    "Our hypothesis seems verified, as there are still some geographical distinctions, but not as obvious from the graphs[^2], or as numerous as previously.  \n",
    "\n",
    "**TODO: voir si ce que je dis sur le corona tient la route**  \n",
    "Note the subhect \"US health\" that we identified as a min. It is to be expected, because \n",
    "**TODO: a word on why \"Sports\" is both a min and a max on different axes**  \n",
    "\n",
    "[^1]:we hypothesize American football in particular, but are not 100% certain.  \n",
    "[^2]: groups of points of one particular color, i.e. corresponding to one consortium in particular are not as obvious when projected on 2 axes, although some still appear (e.g. strange green **TODO** color on projection on axes 0-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d1846",
   "metadata": {},
   "source": [
    "We now focus only on the USA again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9cb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration des graphes avec les USA.\n",
    "\n",
    "# zoom sur 0-4 par ex (pour les vert fluo, qui doivent correspondre à Townsquare Media)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09682eb",
   "metadata": {},
   "source": [
    "As always, we try and identify the topics corresponding to the first axes:\n",
    "\n",
    "| Axis | Max                   | Min                |\n",
    "|------|-----------------------|--------------------|\n",
    "| 0    | Frequent words        | Non frequent words |\n",
    "| 1    | Sports                 | Public health      |\n",
    "| 2    | Music                 | Sports              |\n",
    "| 3    | Local / State related | presidential/political **TODO: verify**       |\n",
    "| 4    | ~Montana state        | not clear **TODO: verify**          |\n",
    "\n",
    "As you can see, they are similar to the previous ones --with unsurprisingly no \"country separation axis\" anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89521e",
   "metadata": {},
   "source": [
    "Visually, the bright green points really stand out.  \n",
    "Can you guess to which journal they correspond?\n",
    "\n",
    "<details>\n",
    "  <summary>Click to reveal</summary>\n",
    "  \n",
    "  Once again, it is Townsquare media which stands out. That's interesting: it is not only divergent with respect to its choice of speaker, but also w.r.t. the subjects it treats.  \n",
    "  We hypothesize it is so particular because it is composed of many *local radio stations*.\n",
    "  Once more we will remove them to refine our analysis.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03247224",
   "metadata": {},
   "source": [
    "Let us remove this consortium from the analysis once again, and see what results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb837df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustration of analysis on words, USA, without Townsquare\n",
    "\n",
    "# zoom on 2-4 (yellow), and maybe 0-2(pale green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e9aa2",
   "metadata": {},
   "source": [
    "The axes we identify are as follow:\n",
    "\n",
    "| Axis | Max                   | Min                |\n",
    "|------|-----------------------|--------------------|\n",
    "| 0    | Frequent words        | Non frequent words |\n",
    "| 1    | Sport                 | Public health      |\n",
    "| 2    | Music                 | Sport              |\n",
    "| 3    | Local / State related | Country wide       |\n",
    "| 4    | Election              | Covid              |\n",
    "\n",
    "We can note than axis 0, 1 and 2 are identic as with the Townsquare media group. Axis 3 is quite similar but less focused on a specific state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fa19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c4d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac54a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ad790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb6e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66c188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344d85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b36be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b487f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d36764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a853f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbfd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf226e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e300b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603cade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942ca62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcf2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071edab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3484c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def0620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dced9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa313f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2bac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de31967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
