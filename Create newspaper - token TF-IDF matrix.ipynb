{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29de8a8",
   "metadata": {},
   "source": [
    "## Create TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3ec6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from Constants import *\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429a6b9",
   "metadata": {},
   "source": [
    "## Create newspaper set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db41f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_newspaper = pd.read_csv(FILE_SPEAKER_NEWSPAPER, compression='bz2') \n",
    "newspapers = set(df_speaker_newspaper.newspaper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0c6c8",
   "metadata": {},
   "source": [
    "## Create token vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c34cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_reader = pd.read_csv(FILE_NEWSPAPER_TOKEN, chunksize=10_000,compression='bz2', converters={\"newspapers\": ast.literal_eval,\"tokens\":ast.literal_eval}) \n",
    "\n",
    "#Create a Counter of all tokens\n",
    "vocabulary = Counter()\n",
    "\n",
    "for (counter, df_chunk) in enumerate(csv_reader):\n",
    "    print(f\"Chunk: {counter}\")\n",
    "    # Count all tokens in the chunk (multiply a token by the number of newspaper quoting the quote)\n",
    "    vocabulary = vocabulary +  Counter(chain.from_iterable(df_chunk.explode(\"newspapers\")[\"tokens\"]))\n",
    "\n",
    "# Removing token that appears in only one newspaper\n",
    "processed_voc = list(np.array(list(vocabulary.keys()))[np.array(list(vocabulary.values()))!=1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad69b9",
   "metadata": {},
   "source": [
    "## Create frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656044fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_reader = pd.read_csv(FILE_NEWSPAPER_TOKEN, chunksize=10_000,compression='bz2', converters={\"newspapers\": ast.literal_eval,\"tokens\":ast.literal_eval}) \n",
    "\n",
    "newspapers = sorted(list(newspapers))\n",
    "newspapers_to_index = {n:i for i, n in enumerate(newspapers)}\n",
    "index_to_newspapers = {i:n for i, n in enumerate(newspapers)}\n",
    "token_to_index = {n:i for i, n in enumerate(newspapers)}\n",
    "index_to_token = {i:n for i, n in enumerate(newspapers)}\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy, vocabulary=processed_voc) \n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'newspapers':newspapers})\n",
    "\n",
    "for (counter, df_chunk) in enumerate(csv_reader):\n",
    "    print(f\"Chunk: {counter}\")\n",
    "    \n",
    "    df_exploded = df_chunk.explode(\"newspapers\")\n",
    "    \n",
    "    # Create dataframe with all the tokens per newspaper\n",
    "    df_grouped = df_exploded.groupby(\"newspapers\")[\"tokens\"].apply(sum).reset_index() \n",
    "    \n",
    "    # Join previous dataframe with a dumb dataframe containing all the newspaper as index\n",
    "    # => Add empty newspaper, allow to create frequency matrix with the correct index for newspaper\n",
    "    df_join = df.set_index('newspapers').join(df_grouped.set_index('newspapers'))\n",
    "    df_join[\"tokens\"] = np.where(df_join[\"tokens\"].isna(), [\"\"], df_join[\"tokens\"])\n",
    "    \n",
    "    # Create token frequency vector by newspaper\n",
    "    X = vectorizer.fit_transform(df_join[\"tokens\"])\n",
    "    \n",
    "    # Sum all the token x newspaper frequency matrix\n",
    "    if(counter == 0):\n",
    "        newspaper_token_frequency = X\n",
    "    else:\n",
    "        newspaper_token_frequency += X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148cdb9",
   "metadata": {},
   "source": [
    "## Create TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddd7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "newspaper_token_tfidf = transformer.fit_transform(newspaper_token_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13608642",
   "metadata": {},
   "source": [
    "## Save TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(FILE_NEWSPAPER_TOKEN_TFIDF, newspaper_token_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a57b37",
   "metadata": {},
   "source": [
    "## Example to load TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20e69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper_token_tfidf = sparse.load_npz(FILE_NEWSPAPER_TOKEN_TFIDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
