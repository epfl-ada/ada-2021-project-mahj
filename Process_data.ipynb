{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e4e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Constants import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) #Print full text\n",
    "pd.set_option('display.max_rows', 200) #Print full text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words(\"english\"))\n",
    "en_stopwords.update([s.capitalize() for s in stopwords.words(\"english\")])\n",
    "\n",
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from tld import get_tld\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5743c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url):\n",
    "    \"\"\"\n",
    "    Extract domain of an url\n",
    "    \"\"\"\n",
    "    url_pruned = urlparse(url).netloc\n",
    "    tld = get_tld(url, as_object=True).tld\n",
    "    url_no_tld = url_pruned.replace('.'+tld,\"\")\n",
    "    domain = url_no_tld.split('.')[-1]\n",
    "\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc423b",
   "metadata": {},
   "source": [
    "## Compute basic statistics about the quotes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 10_000\n",
    "\n",
    "json_reader = pd.read_json(FILE_QUOTES,lines=True,chunksize=CHUNK_SIZE,compression='bz2') \n",
    "\n",
    "quotes_count = 0\n",
    "unique_quotes_count = 0\n",
    "not_NONE_quotes_count = 0\n",
    "for (counter, df_chunk) in enumerate(json_reader):\n",
    "    \n",
    "    unique_quotes_count+= len(df_chunk)\n",
    "    df_chunk['quote_counts']= df_chunk['urls'].apply(lambda urls: len(urls))\n",
    "    quotes_count += df_chunk['quote_counts'].sum()\n",
    "    not_NONE_quotes_count += df_chunk[df_chunk['speaker']!='None']['quote_counts'].sum() \n",
    "    \n",
    "print(f'Number of unique quotes {unique_quotes_count}')\n",
    "\n",
    "#We are not using num_occurences but rather the number of urls to quantify the number of quotes because\n",
    "# around 3% of quotes appear multiple times in the same article which increases numOccurences but is not \n",
    "#relevant in this study\n",
    "print(f'Quotes appearance count {quotes_count}')\n",
    "print(f'Quotes appearance count where speaker is not None {not_NONE_quotes_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba9575c",
   "metadata": {},
   "source": [
    "## Speaker pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048577b",
   "metadata": {},
   "source": [
    "### 1) Create speaker-newspaper dataframe\n",
    "df['newspaper','speaker','proba'] with possible duplicates if speaker was cited multiple times by the newspaper.\n",
    "Quotes with the most probable speaker cited as None are kept and removed further down the pipeline if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d07969",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_reader = pd.read_json(FILE_QUOTES,lines=True,chunksize=CHUNK_SIZE,compression='bz2') \n",
    "\n",
    "for (counter, df_chunk) in enumerate(json_reader):\n",
    "\n",
    "    print(f\"Process chunk {counter+1}\")\n",
    "    \n",
    "    df_chunk = df_chunk[[\"probas\",\"urls\",\"speaker\"]]\n",
    "    \n",
    "    df_chunk[\"proba\"] = df_chunk[\"probas\"].apply(lambda probas: float(probas[0][1]))\n",
    "    \n",
    "    df_chunk = df_chunk.explode(\"urls\")\n",
    "    df_chunk[\"newspaper\"] = df_chunk[\"urls\"].apply(extract_domain)\n",
    "    \n",
    "    #We keep None values and remove it further down the pipeline if needed\n",
    "    df_speakers = df_chunk[[\"newspaper\", \"speaker\",\"proba\"]]\n",
    "    \n",
    "    add_header = (counter==0)\n",
    "    write_mode = 'w' if counter == 0 else 'a'\n",
    "    df_speakers.to_csv(FILE_NEWSPAPER_SPEAKER,header = add_header,index=False, mode=write_mode,compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6f8dc",
   "metadata": {},
   "source": [
    "### 2) For each speaker preprocess its name and compute the number of times it appeared in the articles\n",
    "For now, the certainty probability of a quote's speaker being correct is not taken into account and every (newspaper,speaker) pair is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7a3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speaker(speaker):\n",
    "    #Lower case the names and remove all non alpha numeric characters\n",
    "    new_name = speaker.lower()\n",
    "    new_name = re.sub(r'[^A-Za-z0-9 ]+', '', new_name)\n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb06a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous unique speaker count was 218415 and the new unique speaker count is 212147\n",
      "This correspond to a 2.87% reduction\n"
     ]
    }
   ],
   "source": [
    "#The file is small enough to be loaded in memory\n",
    "df = pd.read_csv(FILE_NEWSPAPER_SPEAKER,compression='bz2')\n",
    "\n",
    "previous_speaker_count = len(df['speaker'].unique())\n",
    "df['speaker'] = df['speaker'].apply(process_speaker)\n",
    "processed_speaker_count = len(df['speaker'].unique())\n",
    "\n",
    "print(f'The previous unique speaker count was {previous_speaker_count} and the new unique speaker count is {processed_speaker_count}')\n",
    "print(f'This correspond to a {(previous_speaker_count-processed_speaker_count)/previous_speaker_count:.2%} reduction')\n",
    "\n",
    "#Save to memory\n",
    "df.to_csv(FILE_NEWSPAPER_SPEAKER,header = True,index=False, mode='w',compression='bz2')\n",
    "\n",
    "newspaper_speaker_count = df.groupby(['newspaper','speaker'],as_index=False).count()\\\n",
    "                            .rename(columns = {'proba':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a29855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newspaper</th>\n",
       "      <th>speaker</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1011now</td>\n",
       "      <td>adam morfeld</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1011now</td>\n",
       "      <td>adam schiff</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1011now</td>\n",
       "      <td>adrian smith</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1011now</td>\n",
       "      <td>alexandra brown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1011now</td>\n",
       "      <td>alfonso morales</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434405</th>\n",
       "      <td>zigwheels</td>\n",
       "      <td>srinivas reddy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434406</th>\n",
       "      <td>zigwheels</td>\n",
       "      <td>toby price</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434407</th>\n",
       "      <td>zigwheels</td>\n",
       "      <td>yash aradhya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434408</th>\n",
       "      <td>zip06</td>\n",
       "      <td>mike caruso</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434409</th>\n",
       "      <td>zip06</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2434410 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         newspaper          speaker  count\n",
       "0          1011now     adam morfeld      4\n",
       "1          1011now      adam schiff      2\n",
       "2          1011now     adrian smith      2\n",
       "3          1011now  alexandra brown      1\n",
       "4          1011now  alfonso morales      1\n",
       "...            ...              ...    ...\n",
       "2434405  zigwheels   srinivas reddy      1\n",
       "2434406  zigwheels       toby price      1\n",
       "2434407  zigwheels     yash aradhya      1\n",
       "2434408      zip06      mike caruso     16\n",
       "2434409      zip06             none      2\n",
       "\n",
       "[2434410 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaper_speaker_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75991850",
   "metadata": {},
   "source": [
    "#### Save processed speakers and create newspaper set and indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4983dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper_speaker_count.to_csv(FILE_NEWSPAPER_SPEAKER_COUNT,header = True,index=False, mode='w',compression='bz2')\n",
    "\n",
    "newspapers = set(newspaper_speaker_count.newspaper)\n",
    "newspapers = sorted(list(newspapers))\n",
    "\n",
    "# Newspaper name => row index\n",
    "newspaper_to_index = {s:i for i,s in enumerate(newspapers)}\n",
    "with open(PICKLE_NEWSPAPER_TO_INDEX, 'wb') as handle:\n",
    "    pickle.dump(newspaper_to_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Row index => newspaper name\n",
    "index_to_newspaper = {i:s for i,s in enumerate(newspapers)}\n",
    "with open(PICKLE_INDEX_TO_NEWSPAPER, 'wb') as handle:\n",
    "    pickle.dump(index_to_newspaper, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60405eaa",
   "metadata": {},
   "source": [
    "### 3) Create speaker set and indexes\n",
    "We create the sorted set of speakers and the indexes to link the matrix entries to speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = set(df_newspaper_speaker_count.speaker)\n",
    "speakers.remove(\"none\")\n",
    "speakers = sorted(list(speakers))\n",
    "\n",
    "speaker_to_index = {s:i for i,s in enumerate(speakers)}\n",
    "index_to_speaker = {i:s for i,s in enumerate(speakers)}\n",
    "\n",
    "    \n",
    "with open(PICKLE_SPEAKER_TO_INDEX, 'wb') as handle:\n",
    "    pickle.dump(speaker_to_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(PICKLE_INDEX_TO_SPEAKER, 'wb') as handle:\n",
    "    pickle.dump(index_to_speaker, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4271f",
   "metadata": {},
   "source": [
    "### 4) Create frequency matrix\n",
    "We generate a frequency matrix #newspaper x #speaker: entry (i,j) is the number of time speaker j appears in newspaper i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7792d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n"
     ]
    }
   ],
   "source": [
    "newspaper_speaker_frequency_matrix = dok_matrix((len(newspapers), len(speakers)), dtype=np.uint32)\n",
    "\n",
    "df_newspaper_speaker_count_not_none = df_newspaper_speaker_count[df_newspaper_speaker_count['speaker']!='none']\n",
    "for index, row in df_newspaper_speaker_count_not_none.iterrows():\n",
    "    if index % 10000==0:\n",
    "        print(index)\n",
    "    index_speaker = speaker_to_index[row['speaker']]\n",
    "    index_newspaper = newspaper_to_index[row['newspaper']]\n",
    "    newspaper_speaker_frequency_matrix[index_newspaper, index_speaker] = row[\"count\"]\n",
    "    \n",
    "newspaper_speaker_frequency_csr = newspaper_speaker_frequency_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a15615",
   "metadata": {},
   "source": [
    "### 5) Create TF-IDF matrix and write it to a file\n",
    "Transform the frequency matrix into a TF-IDF matrix. Each row is normalised and each column is scaled by proportionnaly to the number of newspaper in which the speaker is quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39a84ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "newspaper_speaker_tfidf = transformer.fit_transform(newspaper_speaker_frequency_csr)\n",
    "sparse.save_npz(FILE_NEWSPAPER_SPEAKER_TFIDF, newspaper_speaker_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb97e6",
   "metadata": {},
   "source": [
    "## Quotes pipeline\n",
    "\n",
    "### 1) Preprocess \n",
    "\n",
    "We read all the quotes, preprocess them, create token, write a file <newspaper: tokens> (one line per quote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(quote):\n",
    "    \"\"\"\n",
    "    Lower first letter of the sentence\n",
    "    Remove number\n",
    "    Tokenize\n",
    "    Remove stop words and words of len = 1\n",
    "    Lemmatize\n",
    "    Remove stop words and words of len = 1 (again)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Lower first word of the sentence\n",
    "    lower_first_word = lambda tab: \" \".join([tab[0].lower()] + tab[1:])\n",
    "    quote = \" \".join([lower_first_word(sentence.split(\" \")) for sentence in quote.split(\".\")])\n",
    "    \n",
    "    #Remove Numbers\n",
    "    quote = re.sub(r'\\d+', '', quote) \n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    word_tokens = tokenizer.tokenize(quote)\n",
    "            \n",
    "    remove_stop_words = lambda wt: [w for w in wt if not w in en_stopwords and len(w) > 1]\n",
    "    # Remove stop words and single letters\n",
    "    word_tokens = remove_stop_words(word_tokens)\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    # Remove stop words and single letters\n",
    "    word_tokens = remove_stop_words(word_tokens)\n",
    "        \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0c6c8",
   "metadata": {},
   "source": [
    "### 2) Create token vocabulary and indexes\n",
    "We create the sorted set of tokens and the indexes to link the matrix entries to tokens.\n",
    "We iterate over all the quotes to create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93c34cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0\n",
      "Chunk: 1\n",
      "Chunk: 2\n",
      "Chunk: 3\n",
      "Chunk: 4\n",
      "Chunk: 5\n",
      "Chunk: 6\n",
      "Chunk: 7\n",
      "Chunk: 8\n",
      "Chunk: 9\n",
      "Chunk: 10\n",
      "Chunk: 11\n",
      "Chunk: 12\n",
      "Chunk: 13\n",
      "Chunk: 14\n",
      "Chunk: 15\n",
      "Chunk: 16\n",
      "Chunk: 17\n",
      "Chunk: 18\n",
      "Chunk: 19\n",
      "Chunk: 20\n",
      "Chunk: 21\n",
      "Chunk: 22\n",
      "Chunk: 23\n",
      "Chunk: 24\n",
      "Chunk: 25\n",
      "Chunk: 26\n",
      "Chunk: 27\n",
      "Chunk: 28\n",
      "Chunk: 29\n",
      "Chunk: 30\n",
      "Chunk: 31\n",
      "Chunk: 32\n",
      "Chunk: 33\n",
      "Chunk: 34\n",
      "Chunk: 35\n",
      "Chunk: 36\n",
      "Chunk: 37\n",
      "Chunk: 38\n",
      "Chunk: 39\n",
      "Chunk: 40\n",
      "Chunk: 41\n",
      "Chunk: 42\n",
      "Chunk: 43\n",
      "Chunk: 44\n",
      "Chunk: 45\n",
      "Chunk: 46\n",
      "Chunk: 47\n",
      "Chunk: 48\n",
      "Chunk: 49\n",
      "Chunk: 50\n",
      "Chunk: 51\n",
      "Chunk: 52\n",
      "Chunk: 53\n",
      "Chunk: 54\n",
      "Chunk: 55\n",
      "Chunk: 56\n",
      "Chunk: 57\n",
      "Chunk: 58\n",
      "Chunk: 59\n",
      "Chunk: 60\n",
      "Chunk: 61\n",
      "Chunk: 62\n",
      "Chunk: 63\n",
      "Chunk: 64\n",
      "Chunk: 65\n",
      "Chunk: 66\n",
      "Chunk: 67\n",
      "Chunk: 68\n",
      "Chunk: 69\n",
      "Chunk: 70\n",
      "Chunk: 71\n",
      "Chunk: 72\n",
      "Chunk: 73\n",
      "Chunk: 74\n",
      "Chunk: 75\n",
      "Chunk: 76\n",
      "Chunk: 77\n",
      "Chunk: 78\n",
      "Chunk: 79\n",
      "Chunk: 80\n",
      "Chunk: 81\n",
      "Chunk: 82\n",
      "Chunk: 83\n",
      "Chunk: 84\n",
      "Chunk: 85\n",
      "Chunk: 86\n",
      "Chunk: 87\n",
      "Chunk: 88\n",
      "Chunk: 89\n",
      "Chunk: 90\n",
      "Chunk: 91\n",
      "Chunk: 92\n",
      "Chunk: 93\n",
      "Chunk: 94\n",
      "Chunk: 95\n",
      "Chunk: 96\n",
      "Chunk: 97\n",
      "Chunk: 98\n",
      "Chunk: 99\n",
      "Chunk: 100\n",
      "Chunk: 101\n",
      "Chunk: 102\n",
      "Chunk: 103\n",
      "Chunk: 104\n",
      "Chunk: 105\n",
      "Chunk: 106\n",
      "Chunk: 107\n",
      "Chunk: 108\n",
      "Chunk: 109\n",
      "Chunk: 110\n",
      "Chunk: 111\n",
      "Chunk: 112\n",
      "Chunk: 113\n",
      "Chunk: 114\n",
      "Chunk: 115\n",
      "Chunk: 116\n",
      "Chunk: 117\n",
      "Chunk: 118\n",
      "Chunk: 119\n",
      "Chunk: 120\n",
      "Chunk: 121\n",
      "Chunk: 122\n",
      "Chunk: 123\n",
      "Chunk: 124\n",
      "Chunk: 125\n",
      "Chunk: 126\n",
      "Chunk: 127\n",
      "Chunk: 128\n",
      "Chunk: 129\n",
      "Chunk: 130\n",
      "Chunk: 131\n",
      "Chunk: 132\n",
      "Chunk: 133\n",
      "Chunk: 134\n",
      "Chunk: 135\n",
      "Chunk: 136\n",
      "Chunk: 137\n",
      "Chunk: 138\n",
      "Chunk: 139\n",
      "Chunk: 140\n",
      "Chunk: 141\n",
      "Chunk: 142\n",
      "Chunk: 143\n",
      "Chunk: 144\n",
      "Chunk: 145\n",
      "Chunk: 146\n",
      "Chunk: 147\n",
      "Chunk: 148\n",
      "Chunk: 149\n",
      "Chunk: 150\n",
      "Chunk: 151\n",
      "Chunk: 152\n",
      "Chunk: 153\n",
      "Chunk: 154\n",
      "Chunk: 155\n",
      "Chunk: 156\n",
      "Chunk: 157\n",
      "Chunk: 158\n",
      "Chunk: 159\n",
      "Chunk: 160\n",
      "Chunk: 161\n",
      "Chunk: 162\n",
      "Chunk: 163\n",
      "Chunk: 164\n",
      "Chunk: 165\n",
      "Chunk: 166\n",
      "Chunk: 167\n",
      "Chunk: 168\n",
      "Chunk: 169\n",
      "Chunk: 170\n",
      "Chunk: 171\n",
      "Chunk: 172\n",
      "Chunk: 173\n",
      "Chunk: 174\n",
      "Chunk: 175\n",
      "Chunk: 176\n",
      "Chunk: 177\n",
      "Chunk: 178\n",
      "Chunk: 179\n",
      "Chunk: 180\n",
      "Chunk: 181\n",
      "Chunk: 182\n",
      "Chunk: 183\n",
      "Chunk: 184\n",
      "Chunk: 185\n",
      "Chunk: 186\n",
      "Chunk: 187\n",
      "Chunk: 188\n",
      "Chunk: 189\n",
      "Chunk: 190\n",
      "Chunk: 191\n",
      "Chunk: 192\n",
      "Chunk: 193\n",
      "Chunk: 194\n",
      "Chunk: 195\n",
      "Chunk: 196\n",
      "Chunk: 197\n",
      "Chunk: 198\n",
      "Chunk: 199\n",
      "Chunk: 200\n",
      "Chunk: 201\n",
      "Chunk: 202\n",
      "Chunk: 203\n",
      "Chunk: 204\n",
      "Chunk: 205\n",
      "Chunk: 206\n",
      "Chunk: 207\n",
      "Chunk: 208\n",
      "Chunk: 209\n",
      "Chunk: 210\n",
      "Chunk: 211\n",
      "Chunk: 212\n",
      "Chunk: 213\n",
      "Chunk: 214\n",
      "Chunk: 215\n",
      "Chunk: 216\n",
      "Chunk: 217\n",
      "Chunk: 218\n",
      "Chunk: 219\n",
      "Chunk: 220\n",
      "Chunk: 221\n",
      "Chunk: 222\n",
      "Chunk: 223\n",
      "Chunk: 224\n",
      "Chunk: 225\n",
      "Chunk: 226\n",
      "Chunk: 227\n",
      "Chunk: 228\n",
      "Chunk: 229\n",
      "Chunk: 230\n",
      "Chunk: 231\n",
      "Chunk: 232\n",
      "Chunk: 233\n",
      "Chunk: 234\n",
      "Chunk: 235\n",
      "Chunk: 236\n",
      "Chunk: 237\n",
      "Chunk: 238\n",
      "Chunk: 239\n",
      "Chunk: 240\n",
      "Chunk: 241\n",
      "Chunk: 242\n",
      "Chunk: 243\n",
      "Chunk: 244\n",
      "Chunk: 245\n",
      "Chunk: 246\n",
      "Chunk: 247\n",
      "Chunk: 248\n",
      "Chunk: 249\n",
      "Chunk: 250\n",
      "Chunk: 251\n",
      "Chunk: 252\n",
      "Chunk: 253\n",
      "Chunk: 254\n",
      "Chunk: 255\n",
      "Chunk: 256\n",
      "Chunk: 257\n",
      "Chunk: 258\n",
      "Chunk: 259\n",
      "Chunk: 260\n",
      "Chunk: 261\n",
      "Chunk: 262\n",
      "Chunk: 263\n",
      "Chunk: 264\n",
      "Chunk: 265\n",
      "Chunk: 266\n",
      "Chunk: 267\n",
      "Chunk: 268\n",
      "Chunk: 269\n",
      "Chunk: 270\n",
      "Chunk: 271\n",
      "Chunk: 272\n",
      "Chunk: 273\n",
      "Chunk: 274\n",
      "Chunk: 275\n",
      "Chunk: 276\n",
      "Chunk: 277\n",
      "Chunk: 278\n",
      "Chunk: 279\n",
      "Chunk: 280\n",
      "Chunk: 281\n",
      "Chunk: 282\n",
      "Chunk: 283\n",
      "Chunk: 284\n",
      "Chunk: 285\n",
      "Chunk: 286\n",
      "Chunk: 287\n",
      "Chunk: 288\n",
      "Chunk: 289\n",
      "Chunk: 290\n",
      "Chunk: 291\n",
      "Chunk: 292\n",
      "Chunk: 293\n",
      "Chunk: 294\n",
      "Chunk: 295\n",
      "Chunk: 296\n",
      "Chunk: 297\n",
      "Chunk: 298\n",
      "Chunk: 299\n",
      "Chunk: 300\n",
      "Chunk: 301\n",
      "Chunk: 302\n",
      "Chunk: 303\n",
      "Chunk: 304\n",
      "Chunk: 305\n",
      "Chunk: 306\n",
      "Chunk: 307\n",
      "Chunk: 308\n",
      "Chunk: 309\n",
      "Chunk: 310\n",
      "Chunk: 311\n",
      "Chunk: 312\n",
      "Chunk: 313\n",
      "Chunk: 314\n",
      "Chunk: 315\n",
      "Chunk: 316\n",
      "Chunk: 317\n",
      "Chunk: 318\n",
      "Chunk: 319\n",
      "Chunk: 320\n",
      "Chunk: 321\n",
      "Chunk: 322\n",
      "Chunk: 323\n",
      "Chunk: 324\n",
      "Chunk: 325\n",
      "Chunk: 326\n",
      "Chunk: 327\n",
      "Chunk: 328\n",
      "Chunk: 329\n",
      "Chunk: 330\n",
      "Chunk: 331\n",
      "Chunk: 332\n",
      "Chunk: 333\n",
      "Chunk: 334\n",
      "Chunk: 335\n",
      "Chunk: 336\n",
      "Chunk: 337\n",
      "Chunk: 338\n",
      "Chunk: 339\n",
      "Chunk: 340\n",
      "Chunk: 341\n",
      "Chunk: 342\n",
      "Chunk: 343\n",
      "Chunk: 344\n",
      "Chunk: 345\n",
      "Chunk: 346\n",
      "Chunk: 347\n",
      "Chunk: 348\n",
      "Chunk: 349\n",
      "Chunk: 350\n",
      "Chunk: 351\n",
      "Chunk: 352\n",
      "Chunk: 353\n",
      "Chunk: 354\n",
      "Chunk: 355\n",
      "Chunk: 356\n",
      "Chunk: 357\n",
      "Chunk: 358\n",
      "Chunk: 359\n",
      "Chunk: 360\n",
      "Chunk: 361\n",
      "Chunk: 362\n",
      "Chunk: 363\n",
      "Chunk: 364\n",
      "Chunk: 365\n",
      "Chunk: 366\n",
      "Chunk: 367\n",
      "Chunk: 368\n",
      "Chunk: 369\n",
      "Chunk: 370\n",
      "Chunk: 371\n",
      "Chunk: 372\n",
      "Chunk: 373\n",
      "Chunk: 374\n",
      "Chunk: 375\n",
      "Chunk: 376\n",
      "Chunk: 377\n",
      "Chunk: 378\n",
      "Chunk: 379\n",
      "Chunk: 380\n",
      "Chunk: 381\n",
      "Chunk: 382\n",
      "Chunk: 383\n",
      "Chunk: 384\n",
      "Chunk: 385\n",
      "Chunk: 386\n",
      "Chunk: 387\n",
      "Chunk: 388\n",
      "Chunk: 389\n",
      "Chunk: 390\n",
      "Chunk: 391\n",
      "Chunk: 392\n",
      "Chunk: 393\n",
      "Chunk: 394\n",
      "Chunk: 395\n",
      "Chunk: 396\n",
      "Chunk: 397\n",
      "Chunk: 398\n",
      "Chunk: 399\n",
      "Chunk: 400\n",
      "Chunk: 401\n",
      "Chunk: 402\n",
      "Chunk: 403\n",
      "Chunk: 404\n",
      "Chunk: 405\n",
      "Chunk: 406\n",
      "Chunk: 407\n",
      "Chunk: 408\n",
      "Chunk: 409\n",
      "Chunk: 410\n",
      "Chunk: 411\n",
      "Chunk: 412\n",
      "Chunk: 413\n",
      "Chunk: 414\n",
      "Chunk: 415\n",
      "Chunk: 416\n",
      "Chunk: 417\n",
      "Chunk: 418\n",
      "Chunk: 419\n",
      "Chunk: 420\n",
      "Chunk: 421\n",
      "Chunk: 422\n",
      "Chunk: 423\n",
      "Chunk: 424\n",
      "Chunk: 425\n",
      "Chunk: 426\n",
      "Chunk: 427\n",
      "Chunk: 428\n",
      "Chunk: 429\n",
      "Chunk: 430\n",
      "Chunk: 431\n",
      "Chunk: 432\n",
      "Chunk: 433\n",
      "Chunk: 434\n",
      "Chunk: 435\n",
      "Chunk: 436\n",
      "Chunk: 437\n",
      "Chunk: 438\n",
      "Chunk: 439\n",
      "Chunk: 440\n",
      "Chunk: 441\n",
      "Chunk: 442\n",
      "Chunk: 443\n",
      "Chunk: 444\n",
      "Chunk: 445\n",
      "Chunk: 446\n",
      "Chunk: 447\n",
      "Chunk: 448\n",
      "Chunk: 449\n",
      "Chunk: 450\n",
      "Chunk: 451\n",
      "Chunk: 452\n",
      "Chunk: 453\n",
      "Chunk: 454\n",
      "Chunk: 455\n",
      "Chunk: 456\n",
      "Chunk: 457\n",
      "Chunk: 458\n",
      "Chunk: 459\n",
      "Chunk: 460\n",
      "Chunk: 461\n",
      "Chunk: 462\n",
      "Chunk: 463\n",
      "Chunk: 464\n",
      "Chunk: 465\n",
      "Chunk: 466\n",
      "Chunk: 467\n",
      "Chunk: 468\n",
      "Chunk: 469\n",
      "Chunk: 470\n",
      "Chunk: 471\n",
      "Chunk: 472\n",
      "Chunk: 473\n",
      "Chunk: 474\n",
      "Chunk: 475\n",
      "Chunk: 476\n",
      "Chunk: 477\n",
      "Chunk: 478\n",
      "Chunk: 479\n",
      "Chunk: 480\n",
      "Chunk: 481\n",
      "Chunk: 482\n",
      "Chunk: 483\n",
      "Chunk: 484\n",
      "Chunk: 485\n",
      "Chunk: 486\n",
      "Chunk: 487\n",
      "Chunk: 488\n",
      "Chunk: 489\n",
      "Chunk: 490\n",
      "Chunk: 491\n",
      "Chunk: 492\n",
      "Chunk: 493\n",
      "Chunk: 494\n",
      "Chunk: 495\n",
      "Chunk: 496\n",
      "Chunk: 497\n",
      "Chunk: 498\n",
      "Chunk: 499\n",
      "Chunk: 500\n",
      "Chunk: 501\n",
      "Chunk: 502\n",
      "Chunk: 503\n",
      "Chunk: 504\n",
      "Chunk: 505\n",
      "Chunk: 506\n",
      "Chunk: 507\n",
      "Chunk: 508\n",
      "Chunk: 509\n",
      "Chunk: 510\n",
      "Chunk: 511\n",
      "Chunk: 512\n",
      "Chunk: 513\n",
      "Chunk: 514\n",
      "Chunk: 515\n",
      "Chunk: 516\n",
      "Chunk: 517\n",
      "Chunk: 518\n",
      "Chunk: 519\n",
      "Chunk: 520\n",
      "Chunk: 521\n",
      "Chunk: 522\n",
      "Chunk: 523\n",
      "Chunk: 524\n"
     ]
    }
   ],
   "source": [
    "csv_reader = pd.read_csv(FILE_NEWSPAPER_TOKEN, chunksize=10_000,compression='bz2', converters={\"newspapers\": ast.literal_eval,\"tokens\":ast.literal_eval}) \n",
    "\n",
    "#Create a Counter of all tokens\n",
    "vocabulary = Counter()\n",
    "\n",
    "for (counter, df_chunk) in enumerate(csv_reader):\n",
    "    print(f\"Chunk: {counter}\")\n",
    "    # Count all tokens in the chunk (multiply a token by the number of newspaper quoting the quote)\n",
    "    vocabulary = vocabulary +  Counter(chain.from_iterable(df_chunk.explode(\"newspapers\")[\"tokens\"]))\n",
    "\n",
    "# Removing token that appears in only one newspaper\n",
    "processed_voc = list(np.array(list(vocabulary.keys()))[np.array(list(vocabulary.values()))!=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192dc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_voc = sorted(processed_voc)\n",
    "\n",
    "token_to_index = {n:i for i, n in enumerate(sorted_voc)}\n",
    "index_to_token = {i:n for i, n in enumerate(sorted_voc)}\n",
    "\n",
    "with open(PICKLE_TOKEN_TO_INDEX, 'wb') as handle:\n",
    "    pickle.dump(token_to_index,handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(PICKLE_INDEX_TO_TOKEN, 'wb') as handle:\n",
    "    pickle.dump(index_to_token, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad69b9",
   "metadata": {},
   "source": [
    "### 3) Create frequency matrix\n",
    "We generate a frequency matrix #newspaper x #tokens: entry (i,j) is the number of time token j appears in newspaper i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656044fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0\n",
      "Chunk: 1\n",
      "Chunk: 2\n",
      "Chunk: 3\n",
      "Chunk: 4\n",
      "Chunk: 5\n",
      "Chunk: 6\n",
      "Chunk: 7\n",
      "Chunk: 8\n",
      "Chunk: 9\n",
      "Chunk: 10\n",
      "Chunk: 11\n",
      "Chunk: 12\n",
      "Chunk: 13\n",
      "Chunk: 14\n",
      "Chunk: 15\n",
      "Chunk: 16\n",
      "Chunk: 17\n",
      "Chunk: 18\n",
      "Chunk: 19\n",
      "Chunk: 20\n",
      "Chunk: 21\n",
      "Chunk: 22\n",
      "Chunk: 23\n",
      "Chunk: 24\n",
      "Chunk: 25\n",
      "Chunk: 26\n",
      "Chunk: 27\n",
      "Chunk: 28\n",
      "Chunk: 29\n",
      "Chunk: 30\n",
      "Chunk: 31\n",
      "Chunk: 32\n",
      "Chunk: 33\n",
      "Chunk: 34\n",
      "Chunk: 35\n",
      "Chunk: 36\n",
      "Chunk: 37\n",
      "Chunk: 38\n",
      "Chunk: 39\n",
      "Chunk: 40\n",
      "Chunk: 41\n",
      "Chunk: 42\n",
      "Chunk: 43\n",
      "Chunk: 44\n",
      "Chunk: 45\n",
      "Chunk: 46\n",
      "Chunk: 47\n",
      "Chunk: 48\n",
      "Chunk: 49\n",
      "Chunk: 50\n",
      "Chunk: 51\n",
      "Chunk: 52\n",
      "Chunk: 53\n",
      "Chunk: 54\n",
      "Chunk: 55\n",
      "Chunk: 56\n",
      "Chunk: 57\n",
      "Chunk: 58\n",
      "Chunk: 59\n",
      "Chunk: 60\n",
      "Chunk: 61\n",
      "Chunk: 62\n",
      "Chunk: 63\n",
      "Chunk: 64\n",
      "Chunk: 65\n",
      "Chunk: 66\n",
      "Chunk: 67\n",
      "Chunk: 68\n",
      "Chunk: 69\n",
      "Chunk: 70\n",
      "Chunk: 71\n",
      "Chunk: 72\n",
      "Chunk: 73\n",
      "Chunk: 74\n",
      "Chunk: 75\n",
      "Chunk: 76\n",
      "Chunk: 77\n",
      "Chunk: 78\n",
      "Chunk: 79\n",
      "Chunk: 80\n",
      "Chunk: 81\n",
      "Chunk: 82\n",
      "Chunk: 83\n",
      "Chunk: 84\n",
      "Chunk: 85\n",
      "Chunk: 86\n",
      "Chunk: 87\n",
      "Chunk: 88\n",
      "Chunk: 89\n",
      "Chunk: 90\n",
      "Chunk: 91\n",
      "Chunk: 92\n",
      "Chunk: 93\n",
      "Chunk: 94\n",
      "Chunk: 95\n",
      "Chunk: 96\n",
      "Chunk: 97\n",
      "Chunk: 98\n",
      "Chunk: 99\n",
      "Chunk: 100\n",
      "Chunk: 101\n",
      "Chunk: 102\n",
      "Chunk: 103\n",
      "Chunk: 104\n",
      "Chunk: 105\n",
      "Chunk: 106\n",
      "Chunk: 107\n",
      "Chunk: 108\n",
      "Chunk: 109\n",
      "Chunk: 110\n",
      "Chunk: 111\n",
      "Chunk: 112\n",
      "Chunk: 113\n",
      "Chunk: 114\n",
      "Chunk: 115\n",
      "Chunk: 116\n",
      "Chunk: 117\n",
      "Chunk: 118\n",
      "Chunk: 119\n",
      "Chunk: 120\n",
      "Chunk: 121\n",
      "Chunk: 122\n",
      "Chunk: 123\n",
      "Chunk: 124\n",
      "Chunk: 125\n",
      "Chunk: 126\n",
      "Chunk: 127\n",
      "Chunk: 128\n",
      "Chunk: 129\n",
      "Chunk: 130\n",
      "Chunk: 131\n",
      "Chunk: 132\n",
      "Chunk: 133\n",
      "Chunk: 134\n",
      "Chunk: 135\n",
      "Chunk: 136\n",
      "Chunk: 137\n",
      "Chunk: 138\n",
      "Chunk: 139\n",
      "Chunk: 140\n",
      "Chunk: 141\n",
      "Chunk: 142\n",
      "Chunk: 143\n",
      "Chunk: 144\n",
      "Chunk: 145\n",
      "Chunk: 146\n",
      "Chunk: 147\n",
      "Chunk: 148\n",
      "Chunk: 149\n",
      "Chunk: 150\n",
      "Chunk: 151\n",
      "Chunk: 152\n",
      "Chunk: 153\n",
      "Chunk: 154\n",
      "Chunk: 155\n",
      "Chunk: 156\n",
      "Chunk: 157\n",
      "Chunk: 158\n",
      "Chunk: 159\n",
      "Chunk: 160\n",
      "Chunk: 161\n",
      "Chunk: 162\n",
      "Chunk: 163\n",
      "Chunk: 164\n",
      "Chunk: 165\n",
      "Chunk: 166\n",
      "Chunk: 167\n",
      "Chunk: 168\n",
      "Chunk: 169\n",
      "Chunk: 170\n",
      "Chunk: 171\n",
      "Chunk: 172\n",
      "Chunk: 173\n",
      "Chunk: 174\n",
      "Chunk: 175\n",
      "Chunk: 176\n",
      "Chunk: 177\n",
      "Chunk: 178\n",
      "Chunk: 179\n",
      "Chunk: 180\n",
      "Chunk: 181\n",
      "Chunk: 182\n",
      "Chunk: 183\n",
      "Chunk: 184\n",
      "Chunk: 185\n",
      "Chunk: 186\n",
      "Chunk: 187\n",
      "Chunk: 188\n",
      "Chunk: 189\n",
      "Chunk: 190\n",
      "Chunk: 191\n",
      "Chunk: 192\n",
      "Chunk: 193\n",
      "Chunk: 194\n",
      "Chunk: 195\n",
      "Chunk: 196\n",
      "Chunk: 197\n",
      "Chunk: 198\n",
      "Chunk: 199\n",
      "Chunk: 200\n",
      "Chunk: 201\n",
      "Chunk: 202\n",
      "Chunk: 203\n",
      "Chunk: 204\n",
      "Chunk: 205\n",
      "Chunk: 206\n",
      "Chunk: 207\n",
      "Chunk: 208\n",
      "Chunk: 209\n",
      "Chunk: 210\n",
      "Chunk: 211\n",
      "Chunk: 212\n",
      "Chunk: 213\n",
      "Chunk: 214\n",
      "Chunk: 215\n",
      "Chunk: 216\n",
      "Chunk: 217\n",
      "Chunk: 218\n",
      "Chunk: 219\n",
      "Chunk: 220\n",
      "Chunk: 221\n",
      "Chunk: 222\n",
      "Chunk: 223\n",
      "Chunk: 224\n",
      "Chunk: 225\n",
      "Chunk: 226\n",
      "Chunk: 227\n",
      "Chunk: 228\n",
      "Chunk: 229\n",
      "Chunk: 230\n",
      "Chunk: 231\n",
      "Chunk: 232\n",
      "Chunk: 233\n",
      "Chunk: 234\n",
      "Chunk: 235\n",
      "Chunk: 236\n",
      "Chunk: 237\n",
      "Chunk: 238\n",
      "Chunk: 239\n",
      "Chunk: 240\n",
      "Chunk: 241\n",
      "Chunk: 242\n",
      "Chunk: 243\n",
      "Chunk: 244\n",
      "Chunk: 245\n",
      "Chunk: 246\n",
      "Chunk: 247\n",
      "Chunk: 248\n",
      "Chunk: 249\n",
      "Chunk: 250\n",
      "Chunk: 251\n",
      "Chunk: 252\n",
      "Chunk: 253\n",
      "Chunk: 254\n",
      "Chunk: 255\n",
      "Chunk: 256\n",
      "Chunk: 257\n",
      "Chunk: 258\n",
      "Chunk: 259\n",
      "Chunk: 260\n",
      "Chunk: 261\n",
      "Chunk: 262\n",
      "Chunk: 263\n",
      "Chunk: 264\n",
      "Chunk: 265\n",
      "Chunk: 266\n",
      "Chunk: 267\n",
      "Chunk: 268\n",
      "Chunk: 269\n",
      "Chunk: 270\n",
      "Chunk: 271\n",
      "Chunk: 272\n",
      "Chunk: 273\n",
      "Chunk: 274\n",
      "Chunk: 275\n",
      "Chunk: 276\n",
      "Chunk: 277\n",
      "Chunk: 278\n",
      "Chunk: 279\n",
      "Chunk: 280\n",
      "Chunk: 281\n",
      "Chunk: 282\n",
      "Chunk: 283\n",
      "Chunk: 284\n",
      "Chunk: 285\n",
      "Chunk: 286\n",
      "Chunk: 287\n",
      "Chunk: 288\n",
      "Chunk: 289\n",
      "Chunk: 290\n",
      "Chunk: 291\n",
      "Chunk: 292\n",
      "Chunk: 293\n",
      "Chunk: 294\n",
      "Chunk: 295\n",
      "Chunk: 296\n",
      "Chunk: 297\n",
      "Chunk: 298\n",
      "Chunk: 299\n",
      "Chunk: 300\n",
      "Chunk: 301\n",
      "Chunk: 302\n",
      "Chunk: 303\n",
      "Chunk: 304\n",
      "Chunk: 305\n",
      "Chunk: 306\n",
      "Chunk: 307\n",
      "Chunk: 308\n",
      "Chunk: 309\n",
      "Chunk: 310\n",
      "Chunk: 311\n",
      "Chunk: 312\n",
      "Chunk: 313\n",
      "Chunk: 314\n",
      "Chunk: 315\n",
      "Chunk: 316\n",
      "Chunk: 317\n",
      "Chunk: 318\n",
      "Chunk: 319\n",
      "Chunk: 320\n",
      "Chunk: 321\n",
      "Chunk: 322\n",
      "Chunk: 323\n",
      "Chunk: 324\n",
      "Chunk: 325\n",
      "Chunk: 326\n",
      "Chunk: 327\n",
      "Chunk: 328\n",
      "Chunk: 329\n",
      "Chunk: 330\n",
      "Chunk: 331\n",
      "Chunk: 332\n",
      "Chunk: 333\n",
      "Chunk: 334\n",
      "Chunk: 335\n",
      "Chunk: 336\n",
      "Chunk: 337\n",
      "Chunk: 338\n",
      "Chunk: 339\n",
      "Chunk: 340\n",
      "Chunk: 341\n",
      "Chunk: 342\n",
      "Chunk: 343\n",
      "Chunk: 344\n",
      "Chunk: 345\n",
      "Chunk: 346\n",
      "Chunk: 347\n",
      "Chunk: 348\n",
      "Chunk: 349\n",
      "Chunk: 350\n",
      "Chunk: 351\n",
      "Chunk: 352\n",
      "Chunk: 353\n",
      "Chunk: 354\n",
      "Chunk: 355\n",
      "Chunk: 356\n",
      "Chunk: 357\n",
      "Chunk: 358\n",
      "Chunk: 359\n",
      "Chunk: 360\n",
      "Chunk: 361\n",
      "Chunk: 362\n",
      "Chunk: 363\n",
      "Chunk: 364\n",
      "Chunk: 365\n",
      "Chunk: 366\n",
      "Chunk: 367\n",
      "Chunk: 368\n",
      "Chunk: 369\n",
      "Chunk: 370\n",
      "Chunk: 371\n",
      "Chunk: 372\n",
      "Chunk: 373\n",
      "Chunk: 374\n",
      "Chunk: 375\n",
      "Chunk: 376\n",
      "Chunk: 377\n",
      "Chunk: 378\n",
      "Chunk: 379\n",
      "Chunk: 380\n",
      "Chunk: 381\n",
      "Chunk: 382\n",
      "Chunk: 383\n",
      "Chunk: 384\n",
      "Chunk: 385\n",
      "Chunk: 386\n",
      "Chunk: 387\n",
      "Chunk: 388\n",
      "Chunk: 389\n",
      "Chunk: 390\n",
      "Chunk: 391\n",
      "Chunk: 392\n",
      "Chunk: 393\n",
      "Chunk: 394\n",
      "Chunk: 395\n",
      "Chunk: 396\n",
      "Chunk: 397\n",
      "Chunk: 398\n",
      "Chunk: 399\n",
      "Chunk: 400\n",
      "Chunk: 401\n",
      "Chunk: 402\n",
      "Chunk: 403\n",
      "Chunk: 404\n",
      "Chunk: 405\n",
      "Chunk: 406\n",
      "Chunk: 407\n",
      "Chunk: 408\n",
      "Chunk: 409\n",
      "Chunk: 410\n",
      "Chunk: 411\n",
      "Chunk: 412\n",
      "Chunk: 413\n",
      "Chunk: 414\n",
      "Chunk: 415\n",
      "Chunk: 416\n",
      "Chunk: 417\n",
      "Chunk: 418\n",
      "Chunk: 419\n",
      "Chunk: 420\n",
      "Chunk: 421\n",
      "Chunk: 422\n",
      "Chunk: 423\n",
      "Chunk: 424\n",
      "Chunk: 425\n",
      "Chunk: 426\n",
      "Chunk: 427\n",
      "Chunk: 428\n",
      "Chunk: 429\n",
      "Chunk: 430\n",
      "Chunk: 431\n",
      "Chunk: 432\n",
      "Chunk: 433\n",
      "Chunk: 434\n",
      "Chunk: 435\n",
      "Chunk: 436\n",
      "Chunk: 437\n",
      "Chunk: 438\n",
      "Chunk: 439\n",
      "Chunk: 440\n",
      "Chunk: 441\n",
      "Chunk: 442\n",
      "Chunk: 443\n",
      "Chunk: 444\n",
      "Chunk: 445\n",
      "Chunk: 446\n",
      "Chunk: 447\n",
      "Chunk: 448\n",
      "Chunk: 449\n",
      "Chunk: 450\n",
      "Chunk: 451\n",
      "Chunk: 452\n",
      "Chunk: 453\n",
      "Chunk: 454\n",
      "Chunk: 455\n",
      "Chunk: 456\n",
      "Chunk: 457\n",
      "Chunk: 458\n",
      "Chunk: 459\n",
      "Chunk: 460\n",
      "Chunk: 461\n",
      "Chunk: 462\n",
      "Chunk: 463\n",
      "Chunk: 464\n",
      "Chunk: 465\n",
      "Chunk: 466\n",
      "Chunk: 467\n",
      "Chunk: 468\n",
      "Chunk: 469\n",
      "Chunk: 470\n",
      "Chunk: 471\n",
      "Chunk: 472\n",
      "Chunk: 473\n",
      "Chunk: 474\n",
      "Chunk: 475\n",
      "Chunk: 476\n",
      "Chunk: 477\n",
      "Chunk: 478\n",
      "Chunk: 479\n",
      "Chunk: 480\n",
      "Chunk: 481\n",
      "Chunk: 482\n",
      "Chunk: 483\n",
      "Chunk: 484\n",
      "Chunk: 485\n",
      "Chunk: 486\n",
      "Chunk: 487\n",
      "Chunk: 488\n",
      "Chunk: 489\n",
      "Chunk: 490\n",
      "Chunk: 491\n",
      "Chunk: 492\n",
      "Chunk: 493\n",
      "Chunk: 494\n",
      "Chunk: 495\n",
      "Chunk: 496\n",
      "Chunk: 497\n",
      "Chunk: 498\n",
      "Chunk: 499\n",
      "Chunk: 500\n",
      "Chunk: 501\n",
      "Chunk: 502\n",
      "Chunk: 503\n",
      "Chunk: 504\n",
      "Chunk: 505\n",
      "Chunk: 506\n",
      "Chunk: 507\n",
      "Chunk: 508\n",
      "Chunk: 509\n",
      "Chunk: 510\n",
      "Chunk: 511\n",
      "Chunk: 512\n",
      "Chunk: 513\n",
      "Chunk: 514\n",
      "Chunk: 515\n",
      "Chunk: 516\n",
      "Chunk: 517\n",
      "Chunk: 518\n",
      "Chunk: 519\n",
      "Chunk: 520\n",
      "Chunk: 521\n",
      "Chunk: 522\n",
      "Chunk: 523\n",
      "Chunk: 524\n"
     ]
    }
   ],
   "source": [
    "csv_reader = pd.read_csv(FILE_NEWSPAPER_TOKEN, chunksize=10_000,compression='bz2', converters={\"newspapers\": ast.literal_eval,\"tokens\":ast.literal_eval}) \n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy, vocabulary=sorted_voc) \n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'newspapers':newspapers})\n",
    "\n",
    "for (counter, df_chunk) in enumerate(csv_reader):\n",
    "    print(f\"Chunk: {counter}\")\n",
    "    \n",
    "    df_exploded = df_chunk.explode(\"newspapers\")\n",
    "    \n",
    "    # Create dataframe with all the tokens per newspaper\n",
    "    df_grouped = df_exploded.groupby(\"newspapers\")[\"tokens\"].apply(sum).reset_index() \n",
    "    \n",
    "    # Join previous dataframe with a dumb dataframe containing all the newspaper as index\n",
    "    # => Add empty newspaper, allow to create frequency matrix with the correct index for newspaper\n",
    "    df_join = df.set_index('newspapers').join(df_grouped.set_index('newspapers'))\n",
    "    df_join[\"tokens\"] = np.where(df_join[\"tokens\"].isna(), [\"\"], df_join[\"tokens\"])\n",
    "    \n",
    "    # Create token frequency vector by newspaper\n",
    "    X = vectorizer.fit_transform(df_join[\"tokens\"])\n",
    "    \n",
    "    # Sum all the token x newspaper frequency matrix\n",
    "    if(counter == 0):\n",
    "        newspaper_token_frequency = X\n",
    "    else:\n",
    "        newspaper_token_frequency += X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148cdb9",
   "metadata": {},
   "source": [
    "### 4) Create TF-IDF matrix and write it to a file\n",
    "Transform the frequency matrix into a TF-IDF matrix. Each row is normalised and each column is scaled by proportionnaly to the number of newspaper in which the speaker is quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddd7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "newspaper_token_tfidf = transformer.fit_transform(newspaper_token_frequency)\n",
    "sparse.save_npz(FILE_NEWSPAPER_TOKEN_TFIDF, newspaper_token_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
